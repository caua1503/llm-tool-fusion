# Guia de Uso - llm-tool-fusion

## üìñ Vis√£o Geral

O **llm-tool-fusion** √© uma biblioteca que simplifica a cria√ß√£o e o uso de ferramentas (tools) para modelos de linguagem grandes (LLMs). Com ela, voc√™ pode:

- ‚úÖ Definir ferramentas usando decoradores simples
- ‚úÖ Processar automaticamente as chamadas de ferramentas
- ‚úÖ Trabalhar com diferentes frameworks (OpenAI, Ollama, etc.)
- ‚úÖ Executar ferramentas s√≠ncronas e ass√≠ncronas

## üöÄ Instala√ß√£o

```bash
pip install llm-tool-fusion
```

## üìã Uso B√°sico

### 1. Criando um ToolCaller

```python
from llm_tool_fusion import ToolCaller

# Para OpenAI (padr√£o)
manager = ToolCaller()

# Para Ollama
manager = ToolCaller(framework="ollama")
```

### 2. Definindo Ferramentas

#### Ferramenta S√≠ncrona
```python
@manager.tool
def calcular_preco(preco: float, desconto: float) -> float:
    """
    Calcula o pre√ßo final com desconto
    
    Args:
        preco (float): Pre√ßo original
        desconto (float): Percentual de desconto
        
    Returns:
        float: Pre√ßo final
    """
    return preco * (1 - desconto / 100)
```

#### Ferramenta Ass√≠ncrona
```python
@manager.async_tool
async def buscar_clima(cidade: str) -> str:
    """
    Busca informa√ß√µes do clima
    
    Args:
        cidade (str): Nome da cidade
        
    Returns:
        str: Informa√ß√µes do clima
    """
    # Simula√ß√£o de uma chamada ass√≠ncrona
    await asyncio.sleep(1)
    return f"Clima ensolarado em {cidade}"
```

### 3. Registrando Ferramentas Manualmente

```python
def minha_funcao(valor: int) -> int:
    """Multiplica por 2"""
    return valor * 2

# Registro individual
manager.register_tool(minha_funcao)

# Registro em lote
ferramentas = [
    {"function": funcao1, "type": "sync"},
    {"function": funcao2, "type": "async"}
]
manager.register_list_tools(ferramentas)
```

## üîÑ Processamento Autom√°tico

### Configura√ß√£o B√°sica

```python
from openai import OpenAI
from llm_tool_fusion import process_tool_calls

client = OpenAI()

# Fun√ß√£o para fazer chamadas ao LLM
llm_call_fn = lambda model, messages, tools: client.chat.completions.create(
    model=model, 
    messages=messages, 
    tools=tools
)

# Mensagens iniciais
messages = [{"role": "user", "content": "Calcule 100 reais com 20% de desconto"}]

# Primeira chamada
response = client.chat.completions.create(
    model="gpt-4",
    messages=messages,
    tools=manager.get_tools()
)
```

### Processamento Autom√°tico

```python
resposta_final = process_tool_calls(
    response=response,           # Resposta inicial do LLM
    messages=messages,           # Hist√≥rico de mensagens
    tool_caller=manager,         # Inst√¢ncia do ToolCaller
    model="gpt-4",              # Modelo a usar
    llm_call_fn=llm_call_fn,    # Fun√ß√£o de chamada
    verbose=True,               # Logs detalhados
    verbose_time=True,          # M√©tricas de tempo
    clean_messages=True,        # Retorna s√≥ o conte√∫do
    use_async_poll=True,        # Execu√ß√£o paralela
    max_chained_calls=5         # Limite de chamadas
)

print(resposta_final)
```

## ‚öôÔ∏è M√©todos Principais do ToolCaller

### Obten√ß√£o de Informa√ß√µes
```python
# Lista de ferramentas formatadas
tools = manager.get_tools()

# Mapa de ferramentas dispon√≠veis
available_tools = manager.get_map_tools()

# Nomes das ferramentas ass√≠ncronas
async_tools = manager.get_name_async_tools()

# Nomes das ferramentas s√≠ncronas
sync_tools = manager.get_name_tools()

# Framework atual
framework = manager.get_framework()
```

## ‚ö° Par√¢metros de Configura√ß√£o

### process_tool_calls()

| Par√¢metro | Tipo | Obrigat√≥rio | Descri√ß√£o |
|-----------|------|-------------|-----------|
| `response` | Any | ‚úÖ | Resposta inicial do modelo |
| `messages` | List | ‚úÖ | Lista de mensagens |
| `tool_caller` | ToolCaller | ‚úÖ | Inst√¢ncia do gerenciador |
| `model` | str | ‚úÖ | Nome do modelo |
| `llm_call_fn` | Callable | ‚úÖ | Fun√ß√£o de chamada ao LLM |
| `verbose` | bool | ‚ùå | Logs detalhados (padr√£o: False) |
| `verbose_time` | bool | ‚ùå | M√©tricas de tempo (padr√£o: False) |
| `clean_messages` | bool | ‚ùå | Retorna s√≥ conte√∫do (padr√£o: False) |
| `use_async_poll` | bool | ‚ùå | Execu√ß√£o paralela (padr√£o: False) |
| `max_chained_calls` | int | ‚ùå | Limite de chamadas (padr√£o: 5) |

## üöÄ Vers√£o Ass√≠ncrona

Para aplica√ß√µes que necessitam de processamento ass√≠ncrono:

```python
from llm_tool_fusion import process_tool_calls_async

# Fun√ß√£o ass√≠ncrona de chamada
async_llm_call_fn = lambda model, messages, tools: await client.chat.completions.acreate(
    model=model, 
    messages=messages, 
    tools=tools
)

# Processamento ass√≠ncrono
resposta_final = await process_tool_calls_async(
    response=response,
    messages=messages,
    tool_caller=manager,
    model="gpt-4",
    llm_call_fn=async_llm_call_fn,
    use_async_poll=True  # Recomendado para melhor performance
)
```

## üîß Frameworks Suportados

- **OpenAI**: `ToolCaller(framework="openai")`
- **Ollama**: `ToolCaller(framework="ollama")`

## üí° Dicas de Performance

1. **use_async_poll=True**: Para m√∫ltiplas ferramentas ass√≠ncronas
2. **verbose=False**: Em produ√ß√£o para melhor performance
3. **max_chained_calls**: Ajuste conforme necess√°rio
4. **clean_messages=True**: Para respostas mais limpas

## ‚ö†Ô∏è Observa√ß√µes Importantes

- Sempre documente suas ferramentas com docstrings
- Use type hints para melhor integra√ß√£o
- Teste suas ferramentas antes de usar em produ√ß√£o
- Gerencie adequadamente as exce√ß√µes nas suas ferramentas

## üìö Exemplos Completos

Veja os arquivos na pasta `examples/` para exemplos completos com OpenAI e Ollama. 